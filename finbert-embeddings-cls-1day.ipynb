{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLS - 1DayReturns\n",
        "\n",
        "### Use CLS pooling at the token level (i.e. get one vector per chunk) and attention pooling at the chunk level (i.e. one vector per transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fac558cf54a4749a90a2c6d0ede66c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/359 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "027abe855bc14a6ea7263e133f723395",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "697693bf19054800bf3cb8443f2f8fd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e77cd4134bc4e37869cbf31c3ce1039",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os, glob, hashlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "MODEL_NAME = \"yiyanghkust/finbert-pretrain\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "def chunks(text, max_tokens=512, overlap=50):\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=False,\n",
        "        truncation=False,\n",
        "        return_attention_mask=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    out = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        out.append(tokens[start:start + max_tokens])\n",
        "        start += max_tokens - overlap\n",
        "    return out\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def chunk_to_vector(chunk_id_list,batch_size=16):\n",
        "    vecs=[]\n",
        "    for i in range(0,len(chunk_id_list),batch_size):\n",
        "        batch=chunk_id_list[i:i+batch_size]\n",
        "        inputs = [(tokenizer.prepare_for_model(ch,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True)) for ch in batch]\n",
        "    \n",
        "        enc = tokenizer.pad(\n",
        "            inputs,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    #  FIX: ensure batch dimension\n",
        "    if enc[\"input_ids\"].dim() == 1:\n",
        "        enc[\"input_ids\"] = enc[\"input_ids\"].unsqueeze(0)\n",
        "    if enc[\"attention_mask\"].dim() == 1:\n",
        "        enc[\"attention_mask\"] = enc[\"attention_mask\"].unsqueeze(0)\n",
        "    if \"token_type_ids\" in enc and enc[\"token_type_ids\"].dim() == 1:\n",
        "        enc[\"token_type_ids\"] = enc[\"token_type_ids\"].unsqueeze(0)\n",
        "\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    out = encoder(**enc).last_hidden_state          # (B,512,768)\n",
        "    vec = out[:, 0, :]                              # (B,768)  CLS embedding\n",
        "\n",
        "    vecs.append(vec)\n",
        "    vec=torch.cat(vecs,dim=0)\n",
        "    return vec                      # (C,768)\n",
        "\n",
        "def transcript_id(text):\n",
        "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_cache(data, cache_dir, overlap=0):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for i, (transcript, y, fin_features) in enumerate(data):\n",
        "        cid = transcript_id(transcript)\n",
        "        path = os.path.join(cache_dir, f\"{cid}.pt\")\n",
        "        if os.path.exists(path):\n",
        "            continue\n",
        "\n",
        "        chunk_to_vector_list = chunks(transcript, overlap=overlap)\n",
        "        Z=chunk_to_vector(chunk_to_vector_list,batch_size=8)\n",
        "        f=torch.tensor(fin_features,dtype=torch.float16)        \n",
        "        torch.save(\n",
        "            {\"Z\": Z.to(torch.float16), \"fin_features\": f, \"y\": int(y)},\n",
        "            path\n",
        "        )\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"[{i+1}/{len(data)}] cached | files={len(glob.glob(cache_dir+'/*.pt'))}\")\n",
        "\n",
        "    print(f\"Cached {len(data)} transcripts → {cache_dir}\")\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    def __init__(self, cache_dir):\n",
        "        self.paths = glob.glob(os.path.join(cache_dir, \"*.pt\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj = torch.load(self.paths[idx], map_location=\"cpu\")\n",
        "        return obj[\"Z\"].float(), torch.tensor(obj[\"y\"], dtype=torch.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/root'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"model_input_data_v2.csv\")\n",
        "df = df.sort_values(\"adjusted_date\").reset_index(drop=True)\n",
        "\n",
        "dates = np.array(sorted(df[\"adjusted_date\"].unique()))\n",
        "train_end = int(0.75 * len(dates))\n",
        "val_end   = int(0.87 * len(dates))\n",
        "\n",
        "train_df = df[df[\"adjusted_date\"].isin(dates[:train_end])]\n",
        "val_df   = df[df[\"adjusted_date\"].isin(dates[train_end:val_end])]\n",
        "test_df  = df[df[\"adjusted_date\"].isin(dates[val_end:])]\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "y_train_1d = train_df[\"r1d_direction\"]\n",
        "y_val_1d   = val_df[\"r1d_direction\"]\n",
        "y_test_1d  = test_df[\"r1d_direction\"]\n",
        "\n",
        "y_train_5d = train_df[\"r5d_direction\"]\n",
        "y_val_5d   = val_df[\"r5d_direction\"]\n",
        "y_test_5d  = test_df[\"r5d_direction\"]\n",
        "\n",
        "features_base_train = scaler.fit_transform(train_df[[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"]])\n",
        "features_base_val   = scaler.transform(val_df[[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"]])\n",
        "features_base_test  = scaler.transform(test_df[[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"]])\n",
        "\n",
        "train_transcripts = train_df[\"transcript\"].tolist()\n",
        "val_transcripts   = val_df[\"transcript\"].tolist()\n",
        "test_transcripts  = test_df[\"transcript\"].tolist()\n",
        "\n",
        "train_data_5d = list(zip(train_transcripts, y_train_5d, features_base_train))\n",
        "val_data_5d   = list(zip(val_transcripts, y_val_5d, features_base_val))\n",
        "test_data_5d  = list(zip(test_transcripts, y_test_5d, features_base_test))\n",
        "\n",
        "train_data_1d = list(zip(train_transcripts, y_train_1d, features_base_train))\n",
        "val_data_1d   = list(zip(val_transcripts, y_val_1d, features_base_val))\n",
        "test_data_1d  = list(zip(test_transcripts, y_test_1d, features_base_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50/2823] cached | files=43\n",
            "[100/2823] cached | files=90\n",
            "[150/2823] cached | files=138\n",
            "[200/2823] cached | files=182\n",
            "[250/2823] cached | files=209\n",
            "[300/2823] cached | files=257\n",
            "[400/2823] cached | files=313\n",
            "[450/2823] cached | files=355\n",
            "[550/2823] cached | files=408\n",
            "[600/2823] cached | files=430\n",
            "[650/2823] cached | files=456\n",
            "[700/2823] cached | files=504\n",
            "[750/2823] cached | files=554\n",
            "[800/2823] cached | files=600\n",
            "[850/2823] cached | files=646\n",
            "[900/2823] cached | files=692\n",
            "[950/2823] cached | files=738\n",
            "[1000/2823] cached | files=777\n",
            "[1050/2823] cached | files=824\n",
            "[1100/2823] cached | files=865\n",
            "[1150/2823] cached | files=912\n",
            "[1200/2823] cached | files=958\n",
            "[1250/2823] cached | files=1005\n",
            "[1300/2823] cached | files=1053\n",
            "[1350/2823] cached | files=1100\n",
            "[1400/2823] cached | files=1147\n",
            "[1450/2823] cached | files=1191\n",
            "[1500/2823] cached | files=1239\n",
            "[1550/2823] cached | files=1284\n",
            "[1600/2823] cached | files=1332\n",
            "[1650/2823] cached | files=1366\n",
            "[1700/2823] cached | files=1414\n",
            "[1750/2823] cached | files=1462\n",
            "[1800/2823] cached | files=1510\n",
            "[1850/2823] cached | files=1559\n",
            "[1900/2823] cached | files=1594\n",
            "[1950/2823] cached | files=1643\n",
            "[2000/2823] cached | files=1693\n",
            "[2050/2823] cached | files=1741\n",
            "[2100/2823] cached | files=1789\n",
            "[2150/2823] cached | files=1837\n",
            "[2200/2823] cached | files=1886\n",
            "[2250/2823] cached | files=1934\n",
            "[2300/2823] cached | files=1980\n",
            "[2350/2823] cached | files=2028\n",
            "[2400/2823] cached | files=2073\n",
            "[2450/2823] cached | files=2123\n",
            "[2500/2823] cached | files=2172\n",
            "[2550/2823] cached | files=2222\n",
            "[2600/2823] cached | files=2271\n",
            "[2650/2823] cached | files=2320\n",
            "[2700/2823] cached | files=2368\n",
            "[2750/2823] cached | files=2417\n",
            "[2800/2823] cached | files=2465\n",
            "Cached 2823 transcripts → cache/train_CLS_1d\n"
          ]
        }
      ],
      "source": [
        "build_cache(train_data_1d, \"cache/train_CLS_1d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50/402] cached | files=47\n",
            "[100/402] cached | files=96\n",
            "[150/402] cached | files=141\n",
            "[200/402] cached | files=190\n",
            "[250/402] cached | files=234\n",
            "[300/402] cached | files=282\n",
            "[350/402] cached | files=327\n",
            "[400/402] cached | files=369\n",
            "Cached 402 transcripts → cache/test_CLS_1d\n",
            "[50/319] cached | files=48\n",
            "[100/319] cached | files=95\n",
            "[150/319] cached | files=143\n",
            "[200/319] cached | files=191\n",
            "[250/319] cached | files=239\n",
            "[300/319] cached | files=286\n",
            "Cached 319 transcripts → cache/val_CLS_1d\n"
          ]
        }
      ],
      "source": [
        "build_cache(test_data_1d, \"cache/test_CLS_1d\")\n",
        "build_cache(val_data_1d, \"cache/val_CLS_1d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created zip file: cache_train_CLS_1d.zip\n",
            "Created zip file: cache_val_CLS_1d.zip\n",
            "Created zip file: cache_test_CLS_1d.zip\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_cache(cache_dir, zip_path):\n",
        "    \"\"\"\n",
        "    Zips the entire cache directory into a single .zip file.\n",
        "    \"\"\"\n",
        "    assert os.path.exists(cache_dir), f\"{cache_dir} does not exist\"\n",
        "    shutil.make_archive(\n",
        "        base_name=zip_path.replace(\".zip\", \"\"),\n",
        "        format=\"zip\",\n",
        "        root_dir=cache_dir\n",
        "    )\n",
        "    print(f\"Created zip file: {zip_path}\")\n",
        "\n",
        "# Create zip files for each cache directory\n",
        "zip_cache(\"cache/train_CLS_1d\", \"cache_train_CLS_1d.zip\")\n",
        "zip_cache(\"cache/val_CLS_1d\", \"cache_val_CLS_1d.zip\")\n",
        "zip_cache(\"cache/test_CLS_1d\", \"cache_test_CLS_1d.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Using Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import glob, os, torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CachedZDataset(Dataset):\n",
        "    def __init__(self, cache_dir):\n",
        "        self.paths = sorted(glob.glob(os.path.join(cache_dir, \"*.pt\")))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj = torch.load(self.paths[idx], map_location=\"cpu\")\n",
        "        Z = obj[\"Z\"].float()  # (C,768)\n",
        "        y = torch.tensor(obj[\"y\"], dtype=torch.float32)\n",
        "        return Z, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/root'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 2488\n"
          ]
        }
      ],
      "source": [
        "check_cache = CachedZDataset(\"cache/train_CLS_1d\")\n",
        "print(f\"Training samples: {len(check_cache)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def collate_pad(batch):\n",
        "    # batch = [(Z1, y1), (Z2, y2), ...]\n",
        "    Z_list, y_list = zip(*batch)\n",
        "\n",
        "    B = len(Z_list)\n",
        "    dim = Z_list[0].shape[1]\n",
        "    C_max = max(z.shape[0] for z in Z_list)\n",
        "\n",
        "    Z_pad = torch.zeros(B, C_max, dim)\n",
        "    mask  = torch.zeros(B, C_max)\n",
        "\n",
        "    for i, Z in enumerate(Z_list):\n",
        "        C = Z.shape[0]\n",
        "        Z_pad[i, :C] = Z\n",
        "        mask[i, :C] = 1.0\n",
        "\n",
        "    y = torch.tensor(y_list, dtype=torch.float32)\n",
        "    return Z_pad, mask, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    CachedZDataset(\"cache/train_CLS_1d\"),\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_pad\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    CachedZDataset(\"cache/val_CLS_1d\"),\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad\n",
        ")\n",
        "\n",
        "test_loader=DataLoader(\n",
        "    CachedZDataset(\"cache/test_CLS_1d\"),\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MeanPoolClassifier(nn.Module):\n",
        "    def __init__(self, dim=768, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Z, mask):\n",
        "        # Z: (B,C,768), mask: (B,C)\n",
        "        mask3 = mask.unsqueeze(-1)  # (B,C,1)\n",
        "        doc = (Z * mask3).sum(dim=1) / mask3.sum(dim=1).clamp(min=1e-9)\n",
        "        x = F.relu(self.fc1(doc))\n",
        "        x = self.drop(x)\n",
        "        return self.fc2(x).squeeze(-1)  # (B,)\n",
        "\n",
        "class AttnPoolClassifier(nn.Module):\n",
        "    def __init__(self, dim=768, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Parameter(torch.randn(dim) * 0.02)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Z, mask):\n",
        "        # Z: (B,C,768)\n",
        "        scores = torch.einsum(\"bcd,d->bc\", Z, self.attn)  # (B,C)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1)\n",
        "        doc = torch.einsum(\"bc,bcd->bd\", alpha, Z)        # (B,768)\n",
        "        x = F.relu(self.fc1(doc))\n",
        "        x = self.drop(x)\n",
        "        return self.fc2(x).squeeze(-1)\n",
        "\n",
        "class AttnMLPPoolClassifier(nn.Module):\n",
        "    def __init__(self, dim=768, attn_hidden=256, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(dim, attn_hidden)\n",
        "        self.v = nn.Linear(attn_hidden, 1, bias=False)\n",
        "\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Z, mask):\n",
        "        # Z: (B,C,768), mask: (B,C)\n",
        "        h = torch.tanh(self.W(Z))              # (B,C,H)\n",
        "        scores = self.v(h).squeeze(-1)         # (B,C)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1)   # (B,C)\n",
        "\n",
        "        doc = torch.einsum(\"bc,bcd->bd\", alpha, Z)  # (B,768)\n",
        "\n",
        "        x = F.relu(self.fc1(doc))\n",
        "        x = self.drop(x)\n",
        "        return self.fc2(x).squeeze(-1)\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn as nn\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loop_auc(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss, n = 0.0, 0\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    for Z, mask, y in loader:\n",
        "        Z = Z.to(device, non_blocking=True)\n",
        "        mask = mask.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logit = model(Z, mask)              # (B,)\n",
        "        loss = loss_fn(logit, y)\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        all_logits.append(logit.cpu())\n",
        "        all_labels.append(y.cpu())\n",
        "\n",
        "    avg_loss = total_loss / max(1, n)\n",
        "\n",
        "    logits = torch.cat(all_logits).numpy()\n",
        "    labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    auc = roc_auc_score(labels, logits)\n",
        "\n",
        "    return avg_loss, auc\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_with_early_stopping(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-2,\n",
        "    save_path=\"best.pt\",\n",
        "):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    bad_epochs = 0\n",
        "\n",
        "    best_auc = -float(\"inf\")\n",
        "    patience = 7\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "\n",
        "        for Z, mask, y in train_loader:\n",
        "            Z = Z.to(device, non_blocking=True)\n",
        "            mask = mask.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            logit = model(Z, mask)\n",
        "            loss = loss_fn(logit, y)\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "        val_loss, val_auc = eval_loop_auc(model, val_loader, device)\n",
        "\n",
        "        print(\n",
        "            f\"epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} | \"\n",
        "            f\"val_auc={val_auc:.3f}\"\n",
        "        )\n",
        "    \n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience:\n",
        "                print(\"Early stopping on AUC.\")\n",
        "                break\n",
        "\n",
        "    # load best weights before returning\n",
        "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 01 | train_loss=0.7746 | val_loss=0.6875 | val_auc=0.485\n",
            "epoch 02 | train_loss=0.6599 | val_loss=0.7022 | val_auc=0.525\n",
            "epoch 03 | train_loss=0.6523 | val_loss=0.6914 | val_auc=0.547\n",
            "epoch 04 | train_loss=0.6850 | val_loss=0.6923 | val_auc=0.484\n",
            "epoch 05 | train_loss=0.6975 | val_loss=0.7044 | val_auc=0.529\n",
            "epoch 06 | train_loss=0.7458 | val_loss=0.6881 | val_auc=0.519\n",
            "epoch 07 | train_loss=0.6901 | val_loss=0.6946 | val_auc=0.516\n",
            "epoch 08 | train_loss=0.6747 | val_loss=0.6882 | val_auc=0.536\n",
            "epoch 09 | train_loss=0.6532 | val_loss=0.6969 | val_auc=0.537\n",
            "epoch 10 | train_loss=0.6359 | val_loss=0.6896 | val_auc=0.507\n",
            "Early stopping on AUC.\n",
            "TEST | loss=0.6890 | acc=0.563\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# choose one:\n",
        "mean_pool_model = MeanPoolClassifier().to(device)\n",
        "# attn_pool_model = MeanPoolClassifier().to(device)\n",
        "# model = AttnPoolClassifier().to(device)\n",
        "\n",
        "mean_pool_model_train = train_with_early_stopping(\n",
        "    mean_pool_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    save_path=\"best_mean_pool_1d.pt\",\n",
        ")\n",
        "\n",
        "test_loss, test_auc = eval_loop_auc(mean_pool_model_train, test_loader, device)\n",
        "print(f\"TEST | loss={test_loss:.4f} | acc={test_auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 01 | train_loss=0.6735 | val_loss=0.6856 | val_auc=0.523\n",
            "epoch 02 | train_loss=0.7293 | val_loss=0.6820 | val_auc=0.506\n",
            "epoch 03 | train_loss=0.6914 | val_loss=0.7074 | val_auc=0.528\n",
            "epoch 04 | train_loss=0.6436 | val_loss=0.6837 | val_auc=0.544\n",
            "epoch 05 | train_loss=0.7147 | val_loss=0.6909 | val_auc=0.559\n",
            "epoch 06 | train_loss=0.7318 | val_loss=0.6895 | val_auc=0.544\n",
            "epoch 07 | train_loss=0.6477 | val_loss=0.6915 | val_auc=0.535\n",
            "epoch 08 | train_loss=0.6810 | val_loss=0.6845 | val_auc=0.549\n",
            "epoch 09 | train_loss=0.6715 | val_loss=0.6858 | val_auc=0.556\n",
            "epoch 10 | train_loss=0.7177 | val_loss=0.6968 | val_auc=0.542\n",
            "epoch 11 | train_loss=0.6188 | val_loss=0.6816 | val_auc=0.534\n",
            "epoch 12 | train_loss=0.6082 | val_loss=0.6849 | val_auc=0.541\n",
            "Early stopping on AUC.\n",
            "TEST | loss=0.6931 | auc=0.543\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# choose one:\n",
        "#mean_pool_model = MeanPoolClassifier().to(device)\n",
        "attn_pool_model = AttnPoolClassifier().to(device)\n",
        "# model = AttnPoolClassifier().to(device)\n",
        "\n",
        "model = train_with_early_stopping(\n",
        "    attn_pool_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    save_path=\"best_attn_pool_1d.pt\",\n",
        ")\n",
        "\n",
        "test_loss, test_auc = eval_loop_auc(model, test_loader, device)\n",
        "print(f\"TEST | loss={test_loss:.4f} | auc={test_auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 01 | train_loss=0.6645 | val_loss=0.7218 | val_auc=0.557\n",
            "epoch 02 | train_loss=0.7022 | val_loss=0.6891 | val_auc=0.576\n",
            "epoch 03 | train_loss=0.6585 | val_loss=0.6865 | val_auc=0.555\n",
            "epoch 04 | train_loss=0.6749 | val_loss=0.6982 | val_auc=0.510\n",
            "epoch 05 | train_loss=0.6345 | val_loss=0.6893 | val_auc=0.515\n",
            "epoch 06 | train_loss=0.7048 | val_loss=0.6820 | val_auc=0.557\n",
            "epoch 07 | train_loss=0.7265 | val_loss=0.6868 | val_auc=0.565\n",
            "epoch 08 | train_loss=0.7269 | val_loss=0.6921 | val_auc=0.552\n",
            "epoch 09 | train_loss=0.6394 | val_loss=0.6814 | val_auc=0.549\n",
            "Early stopping on AUC.\n",
            "TEST | loss=0.6919 | auc=0.530\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# choose one:\n",
        "#mean_pool_model = MeanPoolClassifier().to(device)\n",
        "attn_NL_pool_model = AttnMLPPoolClassifier().to(device)\n",
        "# model = AttnPoolClassifier().to(device)\n",
        "\n",
        "model = train_with_early_stopping(\n",
        "    attn_NL_pool_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    save_path=\"best_attn_NL_1d.pt\",\n",
        ")\n",
        "\n",
        "test_loss, test_auc = eval_loop_auc(model, test_loader, device)\n",
        "print(f\"TEST | loss={test_loss:.4f} | auc={test_auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training using embeddings+ finance features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import glob, os, torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CachedZFinDataset(Dataset):\n",
        "    def __init__(self, cache_dir):\n",
        "        self.paths = sorted(glob.glob(os.path.join(cache_dir, \"*.pt\")))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj = torch.load(self.paths[idx], map_location=\"cpu\")\n",
        "        Z = obj[\"Z\"].float()  # (C,768)\n",
        "        y = torch.tensor(obj[\"y\"], dtype=torch.float32)\n",
        "        fin=obj[\"fin_features\"].float()\n",
        "        fin=fin.view(-1)\n",
        "        return Z,fin, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def collate_pad_chunks_with_fin(batch):\n",
        "    # batch: [(Z, fin, y), ...]\n",
        "    Z_list, fin_list, y_list = zip(*batch)\n",
        "\n",
        "    B = len(Z_list)\n",
        "    dim = Z_list[0].shape[1]\n",
        "    C_max = max(z.shape[0] for z in Z_list)\n",
        "\n",
        "    Z_pad = torch.zeros(B, C_max, dim, dtype=torch.float32)\n",
        "    mask  = torch.zeros(B, C_max, dtype=torch.float32)\n",
        "\n",
        "    for i, Z in enumerate(Z_list):\n",
        "        C = Z.shape[0]\n",
        "        Z_pad[i, :C] = Z\n",
        "        mask[i, :C] = 1.0\n",
        "\n",
        "    fin = torch.stack([f.view(-1) for f in fin_list]).float()  # (B,K)\n",
        "    y = torch.tensor(y_list, dtype=torch.float32)              # (B,)\n",
        "\n",
        "    return Z_pad, mask, fin, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    CachedZFinDataset(\"cache/train_CLS_1d\"),\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_pad_chunks_with_fin,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    CachedZFinDataset(\"cache/val_CLS_1d\"),\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad_chunks_with_fin,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    CachedZFinDataset(\"cache/test_CLS_1d\"),\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad_chunks_with_fin,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class AttnPoolTwoTower(nn.Module):\n",
        "    def __init__(self, dim=768, fin_dim=4, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Parameter(torch.randn(dim) * 0.02)\n",
        "\n",
        "        self.doc_proj = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.fin_proj = nn.Sequential(\n",
        "            nn.LayerNorm(fin_dim),          # optional\n",
        "            nn.Linear(fin_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(2 * hidden, 1)\n",
        "\n",
        "    def forward(self, Z, mask, fin):\n",
        "        scores = torch.einsum(\"bcd,d->bc\", Z, self.attn)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1)\n",
        "        doc = torch.einsum(\"bc,bcd->bd\", alpha, Z)  # (B,768)\n",
        "\n",
        "        a = self.doc_proj(doc)   # (B,hidden)\n",
        "        b = self.fin_proj(fin)   # (B,hidden)\n",
        "\n",
        "        x = torch.cat([a, b], dim=1)\n",
        "        return self.out(x).squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn as nn\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loop_auc_fin(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss, n = 0.0, 0\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    for Z, mask,fin, y in loader:\n",
        "        Z = Z.to(device, non_blocking=True)\n",
        "        mask = mask.to(device, non_blocking=True)\n",
        "        fin=fin.to(device,non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logit = model(Z, mask,fin)              # (B,)\n",
        "        loss = loss_fn(logit, y)\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        all_logits.append(logit.cpu())\n",
        "        all_labels.append(y.cpu())\n",
        "\n",
        "    avg_loss = total_loss / max(1, n)\n",
        "\n",
        "    logits = torch.cat(all_logits).numpy()\n",
        "    labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    auc = roc_auc_score(labels, logits)\n",
        "\n",
        "    return avg_loss, auc\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_with_early_stopping_fin(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-2,\n",
        "    save_path=\"best.pt\",\n",
        "):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    bad_epochs = 0\n",
        "\n",
        "    best_auc = -float(\"inf\")\n",
        "    patience = 7\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "\n",
        "        for Z, mask,fin, y in train_loader:\n",
        "            Z = Z.to(device, non_blocking=True)\n",
        "            mask = mask.to(device, non_blocking=True)\n",
        "            fin=fin.to(device,non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            logit = model(Z, mask,fin)\n",
        "            loss = loss_fn(logit, y)\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "        val_loss, val_auc = eval_loop_auc_fin(model, val_loader, device)\n",
        "\n",
        "        print(\n",
        "            f\"epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} | \"\n",
        "            f\"val_auc={val_auc:.3f}\"\n",
        "        )\n",
        "    \n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience:\n",
        "                print(\"Early stopping on AUC.\")\n",
        "                break\n",
        "\n",
        "    # load best weights before returning\n",
        "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 01 | train_loss=0.7541 | val_loss=0.7192 | val_auc=0.472\n",
            "epoch 02 | train_loss=0.5377 | val_loss=0.7033 | val_auc=0.497\n",
            "epoch 03 | train_loss=0.6936 | val_loss=0.7079 | val_auc=0.443\n",
            "epoch 04 | train_loss=0.6381 | val_loss=0.7037 | val_auc=0.460\n",
            "epoch 05 | train_loss=0.6931 | val_loss=0.7123 | val_auc=0.450\n",
            "epoch 06 | train_loss=0.7227 | val_loss=0.7138 | val_auc=0.476\n",
            "epoch 07 | train_loss=0.7068 | val_loss=0.7062 | val_auc=0.468\n",
            "epoch 08 | train_loss=0.7918 | val_loss=0.7051 | val_auc=0.468\n",
            "epoch 09 | train_loss=0.6682 | val_loss=0.7908 | val_auc=0.460\n",
            "Early stopping on AUC.\n",
            "TEST | loss=0.6953 | auc=0.549\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# choose one:\n",
        "#mean_pool_model = MeanPoolClassifier().to(device)\n",
        "attn_poolfin_model = AttnPoolTwoTower().to(device)\n",
        "# model = AttnPoolClassifier().to(device)\n",
        "\n",
        "model = train_with_early_stopping_fin(\n",
        "    attn_poolfin_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    save_path=\"best_attn_pool_1d.pt\",\n",
        ")\n",
        "\n",
        "test_loss, test_auc = eval_loop_auc_fin(model, test_loader, device)\n",
        "print(f\"TEST | loss={test_loss:.4f} | auc={test_auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.46726895734597157 Flipped AUC: 0.5327310426540284\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_auc = eval_loop_auc_fin(model, test_loader, device)\n",
        "print(\"AUC:\", test_auc, \"Flipped AUC:\", 1 - test_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   abret_5d  r5d_direction\n",
            "0 -0.012433              0\n",
            "1 -0.012433              0\n",
            "2 -0.012433              0\n",
            "3 -0.012433              0\n",
            "4 -0.012433              0\n",
            "5 -0.050790              0\n",
            "6 -0.016076              0\n",
            "7  0.028114              1\n",
            "8 -0.017254              0\n",
            "9 -0.019140              0\n",
            "fraction positive abret_5d: 0.503668171557562\n",
            "fraction r5d_direction==1: 0.503668171557562\n"
          ]
        }
      ],
      "source": [
        "print(df[[\"abret_5d\",\"r5d_direction\"]].head(10))\n",
        "print(\"fraction positive abret_5d:\", (df[\"abret_5d\"]>0).mean())\n",
        "print(\"fraction r5d_direction==1:\", (df[\"r5d_direction\"]==1).mean())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
