{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the Python Script\n",
        "\n",
        "Write Python code and collaborate in real time. Your code runs in Modal's\n",
        "**serverless cloud**, and anyone in the same workspace can join.\n",
        "\n",
        "This notebook comes with some common Python libraries installed. Run\n",
        "cells with `Shift+Enter`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "raw_data = pd.read_csv(\"input_data_modal.csv\")\n",
        "CACHE_DIR = \"cache/\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def test_train_split_by_date(df, date_col=\"adjusted_date\", train_frac=0.75, val_frac=0.12):\n",
        "    \"\"\"\n",
        "    Splits the DataFrame into train, validation, and test sets based on unique dates.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: pandas DataFrame containing the data.\n",
        "    - date_col: str, name of the column containing date information.\n",
        "    - train_frac: float, fraction of data to be used for training.\n",
        "    - val_frac: float, fraction of data to be used for validation.\n",
        "    \n",
        "    Returns:\n",
        "    - train_df: DataFrame for training set.\n",
        "    - val_df: DataFrame for validation set.\n",
        "    - test_df: DataFrame for test set.\n",
        "    \"\"\"\n",
        "    # Ensure the DataFrame is sorted by date\n",
        "    df = df.sort_values(date_col).reset_index(drop=True)\n",
        "    \n",
        "    # Get unique sorted dates\n",
        "    dates = np.array(sorted(df[date_col].unique()))\n",
        "    n_dates = len(dates)\n",
        "    \n",
        "    # Calculate split indices\n",
        "    train_end = int(train_frac * n_dates)\n",
        "    val_end   = int((train_frac + val_frac) * n_dates)\n",
        "    \n",
        "    # Split dates\n",
        "    train_dates = dates[:train_end]\n",
        "    val_dates   = dates[train_end:val_end]\n",
        "    test_dates  = dates[val_end:]\n",
        "    \n",
        "    # Create DataFrames for each set\n",
        "    train_df = df[df[date_col].isin(train_dates)].reset_index(drop=True)\n",
        "    val_df   = df[df[date_col].isin(val_dates)].reset_index(drop=True)\n",
        "    test_df  = df[df[date_col].isin(test_dates)].reset_index(drop=True)\n",
        "\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def scale_features(train_df, val_df, test_df, feature_cols):\n",
        "    \"\"\"\n",
        "    Scales the specified feature columns to have mean 0 and standard deviation 1.\n",
        "    \n",
        "    Parameters:\n",
        "    - train_df: DataFrame for training set.\n",
        "    - val_df: DataFrame for validation set.\n",
        "    - test_df: DataFrame for test set.\n",
        "    - feature_cols: list of str, names of the columns to be scaled.\n",
        "    \n",
        "    Returns:\n",
        "    - features_train: numpy array of scaled features for training set.\n",
        "    - features_val: numpy array of scaled features for validation set.\n",
        "    - features_test: numpy array of scaled features for test set.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # Fit scaler on training data and transform\n",
        "    features_train = scaler.fit_transform(train_df[feature_cols])\n",
        "    \n",
        "    # Transform validation and test data\n",
        "    features_val   = scaler.transform(val_df[feature_cols])\n",
        "    features_test  = scaler.transform(test_df[feature_cols])\n",
        "    \n",
        "    return features_train, features_val, features_test"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os, glob, hashlib, shutil\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "def setup_finbert(model_name = \"yiyanghkust/finbert-tone\"):\n",
        "    \"\"\"\n",
        "    Sets up the FinBERT model and tokenizer for embedding financial text.\n",
        "\n",
        "    Returns:\n",
        "        model: The FinBERT model.\n",
        "        tokenizer: The FinBERT tokenizer.\n",
        "        device: torch.device\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    encoder.to(device)\n",
        "    encoder.eval()\n",
        "\n",
        "    for param in encoder.parameters():  # freeze FinBERT weights\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return encoder, tokenizer, device\n",
        "\n",
        "\n",
        "def chunks(text, tokenizer, max_tokens=512, overlap=50):\n",
        "    \"\"\"\n",
        "    Splits the input text into chunks of tokens with specified maximum length (512) and overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked.\n",
        "        tokenizer: The tokenizer to convert text to token IDs.\n",
        "        max_tokens (int): Maximum number of tokens per chunk.\n",
        "        overlap (int): Number of overlapping tokens between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        List of chunks, where each chunk is a list of token IDs.        \n",
        "    \"\"\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=False,\n",
        "        truncation=False,\n",
        "        return_attention_mask=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    out = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        out.append(tokens[start:start + max_tokens])\n",
        "        start += max_tokens - overlap\n",
        "    return out\n",
        "\n",
        "\n",
        "def chunk_to_vector(chunk_id_list, encoder, tokenizer, device, batch_size=16):\n",
        "    \"\"\"\n",
        "    Takes in a list of chunks (each chunk is a list of token IDs), uses FinBERT to compute\n",
        "    CLS vector for each chunk.\n",
        "\n",
        "    Args:\n",
        "        chunk_id_list: List of chunks, where each chunk is a list of token IDs.\n",
        "        encoder: FinBERT model.\n",
        "        tokenizer: FinBERT tokenizer.\n",
        "        device: cpu or gpu device.\n",
        "        batch_size: Number of chunks to process in a batch.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor of shape (num_chunks, hidden_dim)\n",
        "    \"\"\"\n",
        "    vecs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # process in batches and prepare inputs by padding/truncating\n",
        "        for i in range(0, len(chunk_id_list), batch_size):\n",
        "            batch = chunk_id_list[i:i + batch_size]\n",
        "\n",
        "            inputs = [\n",
        "                tokenizer.prepare_for_model(\n",
        "                    ch,\n",
        "                    add_special_tokens=True,\n",
        "                    max_length=512,\n",
        "                    truncation=True,\n",
        "                    return_attention_mask=True\n",
        "                )\n",
        "                for ch in batch\n",
        "            ]\n",
        "\n",
        "            enc = tokenizer.pad(\n",
        "                inputs,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # ensure batch dimension\n",
        "            if enc[\"input_ids\"].dim() == 1:\n",
        "                enc[\"input_ids\"] = enc[\"input_ids\"].unsqueeze(0)\n",
        "            if enc[\"attention_mask\"].dim() == 1:\n",
        "                enc[\"attention_mask\"] = enc[\"attention_mask\"].unsqueeze(0)\n",
        "            if \"token_type_ids\" in enc and enc[\"token_type_ids\"].dim() == 1:\n",
        "                enc[\"token_type_ids\"] = enc[\"token_type_ids\"].unsqueeze(0)\n",
        "\n",
        "            enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "            out = encoder(**enc).last_hidden_state   # (B,512,768)\n",
        "            vec = out[:, 0, :]                       # (B,768) CLS embedding\n",
        "            vecs.append(vec)\n",
        "\n",
        "    vec = torch.cat(vecs, dim=0)\n",
        "    return vec  # (C,768)\n",
        "\n",
        "\n",
        "def transcript_id(text):\n",
        "    \"\"\"\n",
        "    Generates a unique identifier for a given transcript using its MD5 hash.\n",
        "    \"\"\"\n",
        "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "def build_cache(data, cache_dir, encoder, tokenizer, device, overlap=0):\n",
        "    \"\"\"\n",
        "    Takes in a dataset of transcripts, labels, and financial features, computes FinBERT\n",
        "    embeddings, and caches to disk.\n",
        "\n",
        "    Args:\n",
        "        data: List of tuples (transcript, label, fin_features).\n",
        "        cache_dir: Directory to store cached embeddings.\n",
        "        encoder: FinBERT model.\n",
        "        tokenizer: FinBERT tokenizer.\n",
        "        device: cpu or gpu device.\n",
        "        overlap: Number of overlapping tokens between consecutive chunks.\n",
        "\n",
        "    \"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for i, (transcript, y, fin_features) in enumerate(data):\n",
        "        cid = transcript_id(transcript)\n",
        "        path = os.path.join(cache_dir, f\"{cid}.pt\")\n",
        "        if os.path.exists(path):\n",
        "            continue\n",
        "\n",
        "        chunk_id_list = chunks(transcript, tokenizer, overlap=overlap)\n",
        "        Z = chunk_to_vector(chunk_id_list, encoder, tokenizer, device, batch_size=8)\n",
        "\n",
        "        f = torch.tensor(fin_features, dtype=torch.float16)\n",
        "        torch.save(\n",
        "            {\"Z\": Z.to(torch.float16), \"fin_features\": f, \"y\": int(y)},\n",
        "            path\n",
        "        )\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"[{i+1}/{len(data)}] cached | files={len(glob.glob(cache_dir+'/*.pt'))}\")\n",
        "\n",
        "    print(f\"Cached {len(data)} transcripts \u2192 {cache_dir}\")\n",
        "\n",
        "\n",
        "def zip_cache(cache_dir, zip_path):\n",
        "    \"\"\"\n",
        "    Zips the entire cache directory into a single .zip file.\n",
        "    \"\"\"\n",
        "    assert os.path.exists(cache_dir), f\"{cache_dir} does not exist\"\n",
        "    shutil.make_archive(\n",
        "        base_name=zip_path.replace(\".zip\", \"\"),\n",
        "        format=\"zip\",\n",
        "        root_dir=cache_dir\n",
        "    )\n",
        "    print(f\"Created zip file: {zip_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=1,overlap=50):\n",
        "    \"\"\"\n",
        "    High-level function to create FinBERT cache from raw data.\n",
        "    First split data by date into train/val/test, scale financial features, zip them and then input to create finbert cache.\n",
        "\n",
        "    Args:\n",
        "        data: List of tuples (transcript, label, fin_features).\n",
        "        cache_dir: Directory to store cached embeddings.\n",
        "        overlap: Number of overlapping tokens between consecutive chunks.\n",
        "    \"\"\"\n",
        "    encoder, tokenizer, device = setup_finbert()\n",
        "\n",
        "    train_data, val_data, test_data = test_train_split_by_date(raw_data)\n",
        "    train_features,val_features, test_features = scale_features(train_data, val_data, test_data,[\"abvol_20d\", \"abcallday_r1\", \"abcallday_r5\", \"abcallday_r20\"])\n",
        "    train_transcripts,val_transcripts,test_transcripts=train_data[\"transcript\"].tolist(),val_data[\"transcript\"].tolist(),test_data[\"transcript\"].tolist()\n",
        "    \n",
        "    if return_days==1:\n",
        "        y_train_1d,y_val_1d,y_tet_1d=train_data[\"r1d_direction\"],val_data[\"r1d_direction\"],test_data[\"r1d_direction\"]\n",
        "    else:\n",
        "        y_train_1d,y_val_1d,y_tet_1d=train_data[\"r5d_direction\"],val_data[\"r5d_direction\"],test_data[\"r5d_direction\"]\n",
        "    \n",
        "    train_data, val_data, test_data = list(zip(train_transcripts, y_train_1d, train_features)), list(zip(val_transcripts, y_val_1d, val_features)), list(zip(test_transcripts, y_tet_1d, test_features))\n",
        "    \n",
        "    for data, split in zip([train_data, val_data, test_data], [\"train\", \"val\", \"test\"]):\n",
        "        split_cache_dir = os.path.join(cache_dir, split,str(return_days))\n",
        "        zip_path=os.path.join(cache_dir, f\"cache_{split}_{return_days}\"+\".zip\")    \n",
        "        build_cache(data, split_cache_dir, encoder, tokenizer, device, overlap=overlap)\n",
        "        zip_cache(split_cache_dir, zip_path)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     raw_data = prepare_earnings_data()\n",
        "#     create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=1,overlap=50)\n",
        "#     create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=5,overlap=50)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "create_finbert_cache(raw_data,cache_dir=CACHE_DIR,return_days=1,overlap=50)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfde6ad4a30b43009d4b9955e2dd3aed",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbadc38c516648c596f15e71a1d09ffc",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c772a72b5bdd4450b304bcacaa7c75b9",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a7fce85270f4dfd8c8ccc14313f5b63",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50/2316] cached | files=43\n",
            "[100/2316] cached | files=90\n",
            "[150/2316] cached | files=138\n",
            "[200/2316] cached | files=182\n",
            "[250/2316] cached | files=209\n",
            "[300/2316] cached | files=257\n",
            "[400/2316] cached | files=313\n",
            "[450/2316] cached | files=355\n",
            "[550/2316] cached | files=409\n",
            "[650/2316] cached | files=456\n",
            "[700/2316] cached | files=504\n",
            "[750/2316] cached | files=554\n",
            "[800/2316] cached | files=600\n",
            "[900/2316] cached | files=693\n",
            "[1000/2316] cached | files=777\n",
            "[1050/2316] cached | files=824\n",
            "[1100/2316] cached | files=866\n",
            "[1150/2316] cached | files=911\n",
            "[1200/2316] cached | files=957\n",
            "[1250/2316] cached | files=1004\n",
            "[1350/2316] cached | files=1099\n",
            "[1400/2316] cached | files=1146\n",
            "[1450/2316] cached | files=1190\n",
            "[1500/2316] cached | files=1238\n",
            "[1550/2316] cached | files=1283\n",
            "[1600/2316] cached | files=1329\n",
            "[1650/2316] cached | files=1366\n",
            "[1700/2316] cached | files=1412\n",
            "[1750/2316] cached | files=1460\n",
            "[1800/2316] cached | files=1508\n",
            "[1850/2316] cached | files=1557\n",
            "[1950/2316] cached | files=1641\n",
            "[2000/2316] cached | files=1691\n",
            "[2050/2316] cached | files=1740\n",
            "[2100/2316] cached | files=1787\n",
            "[2150/2316] cached | files=1835\n",
            "[2200/2316] cached | files=1884\n",
            "[2250/2316] cached | files=1932\n",
            "[2300/2316] cached | files=1978\n",
            "Cached 2316 transcripts \u2192 cache/train/1\n",
            "Created zip file: cache/cache_train_1.zip\n",
            "[50/515] cached | files=47\n",
            "[100/515] cached | files=94\n",
            "[150/515] cached | files=143\n",
            "[200/515] cached | files=193\n",
            "[250/515] cached | files=242\n",
            "[300/515] cached | files=291\n",
            "[350/515] cached | files=340\n",
            "[400/515] cached | files=389\n",
            "[450/515] cached | files=436\n",
            "[500/515] cached | files=486\n",
            "Cached 515 transcripts \u2192 cache/val/1\n",
            "Created zip file: cache/cache_val_1.zip\n",
            "[50/312] cached | files=48\n",
            "[100/312] cached | files=95\n",
            "[150/312] cached | files=143\n",
            "[200/312] cached | files=190\n",
            "[250/312] cached | files=239\n",
            "[300/312] cached | files=286\n",
            "Cached 312 transcripts \u2192 cache/test/1\n",
            "Created zip file: cache/cache_test_1.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for loading cached FinBERT embeddings and labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, cache_dir):\n",
        "        self.paths = glob.glob(os.path.join(cache_dir, \"*.pt\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj = torch.load(self.paths[idx], map_location=\"cpu\")\n",
        "        return obj[\"Z\"].float(), torch.tensor(obj[\"y\"], dtype=torch.float32)\n",
        "\n",
        "\n",
        "class CachedZFinDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for loading cached FinBERT embeddings, financial features, and labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, cache_dir):\n",
        "        self.paths = sorted(glob.glob(os.path.join(cache_dir, \"*.pt\")))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj = torch.load(self.paths[idx], map_location=\"cpu\")\n",
        "        Z = obj[\"Z\"].float()  # (C,768)\n",
        "        y = torch.tensor(obj[\"y\"], dtype=torch.float32)\n",
        "        fin = obj[\"fin_features\"].float().view(-1)\n",
        "        return Z, fin, y\n",
        "    \n",
        "\n",
        "def collate_pad(batch):\n",
        "    \"\"\"Pads variable-length sequences in the batch with zeros to help with batching.\"\"\" \n",
        "    \n",
        "    # batch = [(Z1, y1), (Z2, y2), ...]\n",
        "    Z_list, y_list = zip(*batch)\n",
        "\n",
        "    B = len(Z_list)\n",
        "    dim = Z_list[0].shape[1]\n",
        "    C_max = max(z.shape[0] for z in Z_list)\n",
        "\n",
        "    Z_pad = torch.zeros(B, C_max, dim)\n",
        "    mask  = torch.zeros(B, C_max)\n",
        "\n",
        "    for i, Z in enumerate(Z_list):\n",
        "        C = Z.shape[0]\n",
        "        Z_pad[i, :C] = Z\n",
        "        mask[i, :C] = 1.0\n",
        "\n",
        "    y = torch.tensor(y_list, dtype=torch.float32)\n",
        "    return Z_pad, mask, y\n",
        "\n",
        "\n",
        "def collate_pad_chunks_with_fin(batch):\n",
        "    \"\"\"Pads variable-length sequences in the batch with zeros to help with batching. Also stacks financial features.\"\"\"\n",
        "    # batch: [(Z, fin, y), ...]\n",
        "    Z_list, fin_list, y_list = zip(*batch)\n",
        "\n",
        "    B = len(Z_list)\n",
        "    dim = Z_list[0].shape[1]\n",
        "    C_max = max(z.shape[0] for z in Z_list)\n",
        "\n",
        "    Z_pad = torch.zeros(B, C_max, dim, dtype=torch.float32)\n",
        "    mask  = torch.zeros(B, C_max, dtype=torch.float32)\n",
        "\n",
        "    for i, Z in enumerate(Z_list):\n",
        "        C = Z.shape[0]\n",
        "        Z_pad[i, :C] = Z\n",
        "        mask[i, :C] = 1.0\n",
        "\n",
        "    fin = torch.stack([f.view(-1) for f in fin_list]).float()  # (B,K)\n",
        "    y = torch.tensor(y_list, dtype=torch.float32)              # (B,)\n",
        "\n",
        "    return Z_pad, mask, fin, y\n",
        "\n",
        "\n",
        "\n",
        "class MeanPoolClassifier(nn.Module):\n",
        "    \"\"\"A simple mean-pooling classifier. Takes the mean of all chunk vectors to return one embedding vector per transcript\n",
        "    \n",
        "    Args:\n",
        "        dim (int): Dimension of input features.\n",
        "        hidden (int): Dimension of hidden layer.\n",
        "        dropout (float): Dropout rate\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output logits of shape (B,). \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim=768, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Z, mask):\n",
        "        # Z: (B,C,768), mask: (B,C)\n",
        "        mask3 = mask.unsqueeze(-1)  # (B,C,1)\n",
        "        doc = (Z * mask3).sum(dim=1) / mask3.sum(dim=1).clamp(min=1e-9)\n",
        "        x = F.relu(self.fc1(doc))\n",
        "        x = self.drop(x)\n",
        "        return self.fc2(x).squeeze(-1)  # (B,)\n",
        "\n",
        "\n",
        "class AttnPoolClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple attention-pooling classifier. Learns attention weights over chunk vectors to return one embedding vector per transcript.\n",
        "    Args:\n",
        "        dim (int): Dimension of input features.\n",
        "        hidden (int): Dimension of hidden layer.\n",
        "        dropout (float): Dropout rate\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output logits of shape (B,).        \n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dim=768, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Parameter(torch.randn(dim) * 0.02)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Z, mask):\n",
        "        # Z: (B,C,768)\n",
        "        scores = torch.einsum(\"bcd,d->bc\", Z, self.attn)  # (B,C)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1)\n",
        "        doc = torch.einsum(\"bc,bcd->bd\", alpha, Z)        # (B,768)\n",
        "        x = F.relu(self.fc1(doc))\n",
        "        x = self.drop(x)\n",
        "        return self.fc2(x).squeeze(-1)\n",
        "\n",
        "class AttnMLPPoolClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention-pooling classifier with non-linearity applied to attention. Learns attention weights over chunk vectors to return one embedding vector per transcript.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimension of input features. \n",
        "        attn_hidden (int): Dimension of attention hidden layer.\n",
        "        hidden (int): Dimension of hidden layer.\n",
        "        dropout (float): Dropout rate\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output logits of shape (B,).        \n",
        "    \"\"\"\n",
        "    def __init__(self, dim=768, attn_hidden=256, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(dim, attn_hidden)\n",
        "        self.v = nn.Linear(attn_hidden, 1, bias=False)\n",
        "\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Z, mask):\n",
        "        # Z: (B,C,768), mask: (B,C)\n",
        "        h = torch.tanh(self.W(Z))              # (B,C,H)\n",
        "        scores = self.v(h).squeeze(-1)         # (B,C)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1)   # (B,C)\n",
        "\n",
        "        doc = torch.einsum(\"bc,bcd->bd\", alpha, Z)  # (B,768)\n",
        "\n",
        "        x = F.relu(self.fc1(doc))\n",
        "        x = self.drop(x)\n",
        "        return self.fc2(x).squeeze(-1)\n",
        "      \n",
        "\n",
        "class AttnPoolTwoTower(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention-pooling two-tower classifier. Learns attention weights over chunk vectors to return one embedding vector per transcript, and combines it with financial features.\n",
        "    Uses two separate projection heads for document and financial features before combining them for final classification.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimension of input features. \n",
        "        fin_dim (int): Dimension of financial features.\n",
        "        hidden (int): Dimension of hidden layer.\n",
        "        dropout (float): Dropout rate\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output logits of shape (B,).        \n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dim=768, fin_dim=4, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Parameter(torch.randn(dim) * 0.02)\n",
        "\n",
        "        self.doc_proj = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.fin_proj = nn.Sequential(\n",
        "            nn.LayerNorm(fin_dim),          # optional\n",
        "            nn.Linear(fin_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(2 * hidden, 1)\n",
        "\n",
        "    def forward(self, Z, mask, fin):\n",
        "        scores = torch.einsum(\"bcd,d->bc\", Z, self.attn)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1)\n",
        "        doc = torch.einsum(\"bc,bcd->bd\", alpha, Z)  # (B,768)\n",
        "\n",
        "        a = self.doc_proj(doc)   # (B,hidden)\n",
        "        b = self.fin_proj(fin)   # (B,hidden)\n",
        "\n",
        "        x = torch.cat([a, b], dim=1)\n",
        "        return self.out(x).squeeze(-1)\n",
        "\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loop_auc(model, loader, device):\n",
        "    \"\"\"\n",
        "    Evaluation loop that computes average loss and AUC over the dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to evaluate.\n",
        "        loader (DataLoader): DataLoader for the evaluation dataset.\n",
        "        device (torch.device): Device to run the evaluation on.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Logits,Average loss and AUC score.        \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss, n = 0.0, 0\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    for Z, mask, y in loader:\n",
        "        Z = Z.to(device, non_blocking=True)\n",
        "        mask = mask.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logit = model(Z, mask)              # (B,)\n",
        "        loss = loss_fn(logit, y)\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        all_logits.append(logit.cpu())\n",
        "        all_labels.append(y.cpu())\n",
        "\n",
        "    avg_loss = total_loss / max(1, n)\n",
        "\n",
        "    logits = torch.cat(all_logits).numpy()\n",
        "    labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    auc = roc_auc_score(labels, logits)\n",
        "\n",
        "    return logits,avg_loss, auc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loop_auc_fin(model, loader, device):\n",
        "    \"\"\"\n",
        "Evaluation loop that computes y_scores, average loss and AUC over the dataset, for models that take financial features and transcript embeddings.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to evaluate.   \n",
        "        loader (DataLoader): DataLoader for the evaluation dataset.\n",
        "        device: Device to run the evaluation on.\n",
        "\n",
        "    Returns:\n",
        "        tuple: y_scores, Average loss and AUC score.        \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss, n = 0.0, 0\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    for Z, mask,fin, y in loader:\n",
        "        Z = Z.to(device, non_blocking=True)\n",
        "        mask = mask.to(device, non_blocking=True)\n",
        "        fin=fin.to(device,non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logit = model(Z, mask,fin)              # (B,)\n",
        "        loss = loss_fn(logit, y)\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        all_logits.append(logit.cpu())\n",
        "        all_labels.append(y.cpu())\n",
        "\n",
        "    avg_loss = total_loss / max(1, n)\n",
        "\n",
        "    logits = torch.cat(all_logits).numpy()\n",
        "    labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    auc = roc_auc_score(labels, logits)\n",
        "\n",
        "    return logits,avg_loss, auc\n",
        "\n",
        "\n",
        "def train_with_early_stopping(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-2,\n",
        "    save_path=\"best.pt\",\n",
        "):\n",
        "    \"\"\"\n",
        "Training loop with early stopping based on validation AUC. Uses AdamW optimizer. \n",
        "Stops training if validation AUC does not improve for a specified number of epochs (patience).\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        device (torch.device): Device to run the training on.\n",
        "        max_epochs (int): Maximum number of epochs to train.\n",
        "        patience (int): Number of epochs to wait for improvement before stopping.\n",
        "        lr (float): Learning rate for the optimizer.\n",
        "        weight_decay (float): Weight decay for the optimizer.\n",
        "        save_path (str): Path to save the best model weights.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The trained model with the best weights loaded.        \n",
        "    \"\"\"\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    bad_epochs = 0\n",
        "\n",
        "    best_auc = -float(\"inf\")\n",
        "    patience = 7\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "\n",
        "        for Z, mask, y in train_loader:\n",
        "            Z = Z.to(device, non_blocking=True)\n",
        "            mask = mask.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            logit = model(Z, mask)\n",
        "            loss = loss_fn(logit, y)\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "        val_logits,val_loss, val_auc = eval_loop_auc(model, val_loader, device)\n",
        "\n",
        "        print(\n",
        "            f\"epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} | \"\n",
        "            f\"val_auc={val_auc:.3f}\"\n",
        "        )\n",
        "    \n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience:\n",
        "                print(\"Early stopping on AUC.\")\n",
        "                break\n",
        "\n",
        "    # load best weights before returning\n",
        "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_with_early_stopping_fin(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=7,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-2,\n",
        "    save_path=\"best.pt\",\n",
        "):\n",
        "    \"\"\"\n",
        "Training loop with early stopping based on validation AUC, for models that take financial features and transcript embeddings. Uses AdamW optimizer. \n",
        "Stops training if validation AUC does not improve for a specified number of epochs (patience).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset. \n",
        "        device (torch.device): Device to run the training on.\n",
        "        max_epochs (int): Maximum number of epochs to train.\n",
        "        patience (int): Number of epochs to wait for improvement before stopping.\n",
        "        lr (float): Learning rate for the optimizer.\n",
        "        weight_decay (float): Weight decay for the optimizer.\n",
        "        save_path (str): Path to save the best model weights.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The trained model with the best weights loaded.\n",
        "    \"\"\"\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    bad_epochs = 0\n",
        "\n",
        "    best_auc = -float(\"inf\")\n",
        "    patience = 7\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "\n",
        "        for Z, mask,fin, y in train_loader:\n",
        "            Z = Z.to(device, non_blocking=True)\n",
        "            mask = mask.to(device, non_blocking=True)\n",
        "            fin=fin.to(device,non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            logit = model(Z, mask,fin)\n",
        "            loss = loss_fn(logit, y)\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "        val_logits,val_loss, val_auc = eval_loop_auc_fin(model, val_loader, device)\n",
        "\n",
        "        print(\n",
        "            f\"epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} | \"\n",
        "            f\"val_auc={val_auc:.3f}\"\n",
        "        )\n",
        "    \n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience:\n",
        "                print(\"Early stopping on AUC.\")\n",
        "                break\n",
        "\n",
        "    # load best weights before returning\n",
        "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_cached_finbert_dataset(split,return_days=1,cache_dir=CACHE_DIR,batch_size=16,shuffle=True):\n",
        "    \"\"\"\n",
        "    Loads a cached FinBERT dataset from disk.\n",
        "\n",
        "    Args:\n",
        "        cache_dir: Directory where cached embeddings are stored.\n",
        "        split: One of \"train\", \"val\", or \"test\".\n",
        "        batch_size: Batch size for DataLoader.\n",
        "        shuffle: Whether to shuffle the data.\n",
        "        collate_fn: Function to collate batches \n",
        "    Returns:\n",
        "        DataLoader for the specified split.\n",
        "    \"\"\"\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    split_cache_dir = os.path.join(cache_dir,split,str(return_days))\n",
        "    dataset = CachedDataset(split_cache_dir)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_pad)\n",
        "    return dataloader\n",
        "\n",
        "def load_cached_finbert_fin_dataset(split,return_days=1,cache_dir=CACHE_DIR,batch_size=16,shuffle=True):\n",
        "    \"\"\"\n",
        "    Loads a cached FinBERT dataset with financial features from disk.\n",
        "\n",
        "    Args:\n",
        "        cache_dir: Directory where cached embeddings are stored.\n",
        "        split: One of \"train\", \"val\", or \"test\".\n",
        "        batch_size: Batch size for DataLoader.\n",
        "        shuffle: Whether to shuffle the data.\n",
        "        collate_fn: Function to collate batches \n",
        "    Returns:\n",
        "        DataLoader for the specified split.\n",
        "    \"\"\"\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    split_cache_dir = os.path.join(cache_dir, split,str(return_days))\n",
        "    dataset = CachedZFinDataset(split_cache_dir)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_pad_chunks_with_fin)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def bootstrap_auc_se(y_true, y_scores, n_bootstraps=1000, random_seed=42):\n",
        "    \"\"\"\n",
        "    Computes bootstrap confidence intervals and standard error for AUC.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True binary labels.\n",
        "        y_scores (array-like): Predicted scores.\n",
        "        n_bootstraps (int): Number of bootstrap samples.\n",
        "        random_seed (int): Random seed for reproducibility.\n",
        "    Returns:\n",
        "        tuple: (lower_bound, upper_bound) of 95% confidence interval for AUC\n",
        "        int: standard error of AUC\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "    bootstrapped_scores = []\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_scores = np.array(y_scores)\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        indices = rng.randint(0, len(y_scores), len(y_scores))\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "        score = roc_auc_score(y_true[indices], y_scores[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    sorted_scores = np.array(bootstrapped_scores)\n",
        "    sorted_scores.sort()\n",
        "\n",
        "    lower_bound = sorted_scores[int(0.025 * len(sorted_scores))]\n",
        "    upper_bound = sorted_scores[int(0.975 * len(sorted_scores))]\n",
        "    se = np.std(bootstrapped_scores)\n",
        "\n",
        "    return (lower_bound, upper_bound), se\n",
        "\n",
        "def call_model(Model=\"AttnMLPPoolClassifier\",dim=768, attn_hidden=256, hidden=256, dropout=0.2,return_period=1):\n",
        "    \"\"\"\n",
        "    trains the specifed model at the given return period and returns the trained model, test loss and test AUC confidence intervals.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_loader = load_cached_finbert_dataset(\n",
        "        cache_dir=CACHE_DIR, split=\"train\",return_days=1 ,batch_size=16, shuffle=True\n",
        "    )\n",
        "    val_loader = load_cached_finbert_dataset(\n",
        "        cache_dir=CACHE_DIR, split=\"val\",return_days=1,batch_size=16, shuffle=False\n",
        "    )\n",
        "    test_loader = load_cached_finbert_dataset(\n",
        "        cache_dir=CACHE_DIR, split=\"test\",return_days=1,batch_size=16, shuffle=False\n",
        "    )\n",
        "\n",
        "    if Model==\"MeanPoolClassifier\":\n",
        "        model = MeanPoolClassifier(dim=dim, hidden=hidden, dropout=dropout).to(device)\n",
        "    elif Model==\"AttnPoolClassifier\":\n",
        "        model = AttnPoolClassifier(dim=dim, hidden=hidden, dropout=dropout).to(device)\n",
        "    elif Model==\"AttnMLPPoolClassifier\":\n",
        "        model = AttnMLPPoolClassifier(dim=dim, attn_hidden=attn_hidden, hidden=hidden, dropout=dropout).to(device)\n",
        "\n",
        "    model = train_with_early_stopping(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        device,\n",
        "        max_epochs=50,\n",
        "        patience=7,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-2,\n",
        "        save_path=f\"best_model_{return_period}r.pt\",\n",
        "    )\n",
        "\n",
        "    test_logits,test_loss, test_auc = eval_loop_auc(model, test_loader, device)\n",
        "\n",
        "    test_auc_ci,test_se = bootstrap_auc_se(\n",
        "        y_true=[y for _, y in test_loader.dataset],\n",
        "        y_scores=test_logits,\n",
        "        n_bootstraps=1000,\n",
        "        random_seed=42,\n",
        "    )\n",
        "\n",
        "    return model, test_loss, test_auc, test_auc_ci,test_se\n",
        "\n",
        "def call_model_fin(Model=\"AttnPoolTwoTower\",dim=768, fin_dim=4, hidden=256, dropout=0.2,return_period=1):\n",
        "    \"\"\"\n",
        "    trains the specifed model at the given return period and returns the trained model, test loss and test AUC.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_loader = load_cached_finbert_fin_dataset(\n",
        "        cache_dir=CACHE_DIR, split=\"train\",return_days=return_period ,batch_size=16, shuffle=True\n",
        "    )\n",
        "    val_loader = load_cached_finbert_fin_dataset(\n",
        "        cache_dir=CACHE_DIR, split=\"val\",return_days=return_period,batch_size=16, shuffle=False\n",
        "    )\n",
        "    test_loader = load_cached_finbert_fin_dataset(\n",
        "        cache_dir=CACHE_DIR, split=\"test\",return_days=return_period,batch_size=16, shuffle=False\n",
        "    )\n",
        "    if Model==\"AttnPoolTwoTower\":\n",
        "        model = AttnPoolTwoTower(dim=768, fin_dim=4, hidden=256, dropout=0.2).to(device)\n",
        "\n",
        "    model = train_with_early_stopping_fin(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        device,\n",
        "        max_epochs=50,\n",
        "        patience=7,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-2,\n",
        "        save_path=f\"best_model_fin_{return_period}r.pt\",\n",
        "    )\n",
        "\n",
        "    test_logits,test_loss, test_auc = eval_loop_auc_fin(model, test_loader, device)\n",
        "\n",
        "    test_auc_ci,test_se = bootstrap_auc_se(\n",
        "        y_true=[y for _, _, y in test_loader.dataset],\n",
        "        y_scores=test_logits,\n",
        "        n_bootstraps=1000,\n",
        "        random_seed=42,\n",
        "    )\n",
        "\n",
        "    return model, test_loss, test_auc, test_auc_ci,test_se\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "model, test_loss, test_auc, test_auc_ci,test_se = call_model()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01 | train_loss=0.7111 | val_loss=0.7000 | val_auc=0.448\n",
            "epoch 02 | train_loss=0.6803 | val_loss=0.7114 | val_auc=0.449\n",
            "epoch 03 | train_loss=0.6952 | val_loss=0.7018 | val_auc=0.446\n",
            "epoch 04 | train_loss=0.7677 | val_loss=0.7076 | val_auc=0.447\n",
            "epoch 05 | train_loss=0.6844 | val_loss=0.7006 | val_auc=0.461\n",
            "epoch 06 | train_loss=0.6850 | val_loss=0.6963 | val_auc=0.453\n",
            "epoch 07 | train_loss=0.7227 | val_loss=0.7091 | val_auc=0.447\n",
            "epoch 08 | train_loss=0.6679 | val_loss=0.6944 | val_auc=0.456\n",
            "epoch 09 | train_loss=0.6831 | val_loss=0.7108 | val_auc=0.462\n",
            "epoch 10 | train_loss=0.7083 | val_loss=0.6967 | val_auc=0.463\n",
            "epoch 11 | train_loss=0.6960 | val_loss=0.7039 | val_auc=0.480\n",
            "epoch 12 | train_loss=0.7070 | val_loss=0.7018 | val_auc=0.474\n",
            "epoch 13 | train_loss=0.7095 | val_loss=0.7128 | val_auc=0.479\n",
            "epoch 14 | train_loss=0.6869 | val_loss=0.7226 | val_auc=0.473\n",
            "epoch 15 | train_loss=0.6671 | val_loss=0.7247 | val_auc=0.477\n",
            "epoch 16 | train_loss=0.7232 | val_loss=0.7287 | val_auc=0.461\n",
            "epoch 17 | train_loss=0.6595 | val_loss=0.7242 | val_auc=0.467\n",
            "epoch 18 | train_loss=0.6346 | val_loss=0.7280 | val_auc=0.472\n",
            "Early stopping on AUC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "test_loss, test_auc, test_auc_ci,test_se"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "(0.7124160014976889,\n 0.47144735606274074,\n (np.float64(0.40213294106830927), np.float64(0.5353311135775065)),\n np.float64(0.03381056244048584))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "model, test_loss, test_auc, test_auc_ci,test_se = call_model_fin()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01 | train_loss=0.6900 | val_loss=0.6921 | val_auc=0.485\n",
            "epoch 02 | train_loss=0.7701 | val_loss=0.7001 | val_auc=0.533\n",
            "epoch 03 | train_loss=0.6255 | val_loss=0.8209 | val_auc=0.526\n",
            "epoch 04 | train_loss=0.7149 | val_loss=0.7168 | val_auc=0.498\n",
            "epoch 05 | train_loss=0.6817 | val_loss=0.7028 | val_auc=0.511\n",
            "epoch 06 | train_loss=0.5825 | val_loss=0.7079 | val_auc=0.505\n",
            "epoch 07 | train_loss=0.6441 | val_loss=0.7294 | val_auc=0.465\n",
            "epoch 08 | train_loss=0.6140 | val_loss=0.7367 | val_auc=0.486\n",
            "epoch 09 | train_loss=0.5509 | val_loss=0.7478 | val_auc=0.497\n",
            "Early stopping on AUC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "test_loss, test_auc, test_auc_ci,test_se"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "(0.7000399997678853,\n 0.4517704517704518,\n (np.float64(0.38404781918371156), np.float64(0.5198935275987672)),\n np.float64(0.03475443969530353))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}